import requests
from bs4 import BeautifulSoup
import imagehash
import json
import os
import threading
from queue import Queue
from PIL import Image
from io import BytesIO

FICHIER_HASHES = r"C:\Users\Chabot\Desktop\NSI\Trophee NSI\crawler.json"
MAX_THREADS = 5
BASE_URL = "https://www.wikiart.org"
RANDOM_URL = f"{BASE_URL}/fr/random"
verrou = threading.Lock()
file_urls = Queue()

def sauvegarder_hashes(nouveaux_hashes):
    """Ajoute les nouveaux hashes au fichier JSON s'ils ne sont pas d√©j√† enregistr√©s."""
    with verrou:
        if os.path.exists(FICHIER_HASHES):
            try:
                with open(FICHIER_HASHES, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except json.JSONDecodeError:
                data = []
        else:
            data = []

        for h in nouveaux_hashes:
            if not any(d["hash"] == h["hash"] for d in data):
                data.append(h)

        with open(FICHIER_HASHES, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4)

def extraire_infos(soup, url_page):
    """Extrait le nom de l'artiste et de l'≈ìuvre √† partir du HTML."""
    try:
        img_tag = soup.find("img", itemprop="image")
        img_url = img_tag["src"] if img_tag else None

        script_tag = soup.find("div", class_="wiki-layout-painting-info-bottom")
        if script_tag:
            json_str = script_tag["ng-init"].split("=", 1)[-1].strip().rstrip(";")
            json_data = json.loads(json_str.replace("&quot;", "\""))
            artiste = json_data.get("artistName", "Inconnu")
            oeuvre = json_data.get("title", "Inconnu")
        else:
            artiste, oeuvre = "Inconnu", "Inconnu"

        return artiste, oeuvre, img_url
    except Exception as e:
        print(f"‚ùå Erreur extraction infos : {e}")
        return "Inconnu", "Inconnu", None

def hacher_image():
    """Fait une requ√™te vers /random, suit la redirection, et extrait les infos de l'≈ìuvre."""
    try:
        r = requests.get(RANDOM_URL, timeout=5, allow_redirects=True)
        if r.status_code != 200:
            return

        url_page = r.url  
        soup = BeautifulSoup(r.content, "html.parser")
        artiste, oeuvre, img_url = extraire_infos(soup, url_page)

        if not img_url:
            print(f"‚ùå Aucune image trouv√©e sur {url_page}")
            return

        img_res = requests.get(img_url, timeout=5)
        if img_res.status_code == 200:
            img = Image.open(BytesIO(img_res.content)).convert("RGB")

            if min(img.size) < 100:
                print(f"‚ùå Image trop petite sur {url_page}")
                return
            
            img_resized = img.resize((256, 256)) 
            hash_image = str(imagehash.average_hash(img_resized))

            data = {"url_page": url_page, "hash": hash_image, "artiste": artiste, "oeuvre": oeuvre}
            sauvegarder_hashes([data])
            print(f"‚úÖ Ajout : {artiste} - {oeuvre} ({url_page})")

    except Exception as e:
        print(f"‚ùå Erreur de traitement :", e)

def worker():
    """Thread qui traite une page al√©atoire."""
    while True:
        file_urls.get()
        hacher_image()
        file_urls.task_done()

def crawler_art():
    """D√©marre le crawler multithread sur des pages al√©atoires."""
    print("üöÄ D√©marrage du crawler...")

    for _ in range(MAX_THREADS * 3):  
        file_urls.put(None)

    print("‚ö° Lancement des threads...")
    threads = [threading.Thread(target=worker, daemon=True) for _ in range(MAX_THREADS)]

    for t in threads:
        t.start()

    file_urls.join()

    for _ in range(MAX_THREADS):
        file_urls.put(None)
    for t in threads:
        t.join()

crawler_art()
